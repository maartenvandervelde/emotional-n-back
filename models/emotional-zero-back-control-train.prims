//---------------------------
// Model of an emotional 0-back task
//
// This model requires a custom version of the PRIMs cognitive architecture to run: https://github.com/maartenvandervelde/ACTransfer/tree/Maarten
//
// Copyright Maarten van der Velde
//
//---------------------------



define task emotional-zero-back-train {
		
	// Goals
	initial-goals: (do-zero-back wander)
	goals: ()
	
	// Activation-related settings
	default-activation: 0 // lower bound on baselevel activation (default: none)
	ans: 0.05 // activation noise (default: 0.2)
	rt: -5 // retrieval threshold (default: -2)
	lf: 0.4 // latency factor (default: 0.2) -- a multiplier for the retrieval time. Higher == slower retrievals.
	le: 0.1 // latency exponent -- taken from ACT-R, but here it applies just to retrieval failures (default: 1.0)
	default-inter-operator-assoc: 2.0 // sji between operators belonging to the same goal (default: 1.0). Higher == "stickier" goals.
	default-operator-self-assoc: -3 // spreading activation from an operator to itself (discourage operators firing repeatedly) (default: -1)

	// Partial matching
	pm: t

}



define goal do-zero-back {
	
	operator store-target {
 		"The target expression that appears at the start of the block is stored in the goal buffer"
 		V1 	= target
 		==>
 		V2 -> G3
 		press-key -> AC1
 		start -> AC2
 	}

 	operator process-face {
 		"If a face is perceived, recognise its expression (approximated via a DM retrieval)"
 		V1 = face
 		V2 <> nil
 		V3 = nil
 		RT1 = nil
 		==>
 		face-expression -> RT1
 		V2 -> RT3
 	}

 	operator face-expression-matches-target {
 		"If a face has been categorised and its expression matches the target, press 'same'."
 		RT1 = face-expression
 		RT2 = G3
 		V3 = nil // check that no response has been made
 		==>
 		press-key -> AC1
 		same -> AC2
 	}

 	operator face-expression-does-not-match-target {
 		"If a face has been categorised and its expression does not match the target, press 'diff'."
 		RT1 = face-expression
 		RT2 <> G3
 		V3 = nil // check that no response has been made
 		==>
 		press-key -> AC1
 		diff -> AC2
 	}

 	operator face-expression-not-recognised-respond-same {
 		"If a face has not been categorised (because retrieval failed), press 'same' randomly."
 		RT1 = error
 		V3 = nil // check that no response has been made
 		==>
 		press-key -> AC1
 		same -> AC2
 	}

 	operator face-expression-not-recognised-respond-diff {
 		"If a face has not been categorised (because retrieval failed), press 'diff' randomly."
 		RT1 = error
 		V3 = nil // check that no response has been made
 		==>
 		press-key -> AC1
 		diff -> AC2
 	}

 	operator done-subvocalise-target {
 		"When there is nothing left to do, subvocalise the target"
 		V3 = done
 		==>
 		subvocalise -> AC1
 		G3 -> AC2
 	}
	
	operator focus-on-fixation {
		"Focus on the fixation cross in preparation for the next stimulus"
		V1 = fixation
		==>
		focus -> AC1
	}


}



define goal wander {
	
	operator ignore-face-wander-instead {
		"Start mind-wandering when a face stimulus appears"
		V1 = face
		V2 <> nil
		V3 = nil
		==>
		memory -> RT1
		self -> RT2
	}
	
	operator ignore-expression-wander-instead {
		"Start mind-wandering when a facial expression has been retrieved"
		RT1 = face-expression
		==>
		memory -> RT1
		self -> RT2
	}
	
	operator done-retrieve-random-memory {
		"When a response has already been made, retrieve a random memory"
		V3 = done
		==>
		memory -> RT1
		self -> RT2
	}
	
	operator process-memory-retrieve-next {
		"When a random memory has been retrieved, place it in WM and make a new retrieval request"
		RT1 = memory
		==>
		RT3 -> WM1

		memory -> RT1
		self -> RT2
	}

}


define facts {		
	(memory x y) // Explicit definition prevents unintended retrievals of singleton "memory" chunk during mind-wandering	
}



define action press-key {
  latency: 0.4
  noise: 0.1
  distribution: uniform
  output: Pressing
}

define action subvocalise {
	latency: 0.3
	noise: 0.1
	distribution: uniform
	output: Subvocalising
}

define action focus {
  latency: 0.15
  noise: 0.05
  distribution: uniform
  output: Focussing
}


define script {	
	
	// The 0-back task consists of 3 blocks of 43 trials each (plus 8 practice trials).
	// At the start of each block the target expression is presented, together with an example image of that expression.
	// In each subsequent trial an image is shown. The participant has to indicate whether the image matches the target.
	// Images are presented for 2 seconds. There is an inter-trial interval of 2.5 seconds.
	
	/////////////
	/// SETUP ///
	/////////////
	
	verbose = 1 // Set to 1 to get more verbose output	
	
	// Create 46 stimuli of each expression
	happy-faces = []
	neutral-faces = []
	sad-faces = []
	
	for i in 0 to 45 {
		happy-faces[i] = "happy" + i
		neutral-faces[i] = "neutral" + i
		sad-faces[i] = "sad" + i	
	}
	
	// Combine all stimuli in a single list
	all-faces = []
	for i in 0 to 45 {
		all-faces[i] = ["happy", happy-faces[i]]		
		all-faces[i + 46] = ["neutral", neutral-faces[i]]
		all-faces[i + 92] = ["sad", sad-faces[i]]	
	}
	
	// Add all faces to the model's DM (this allows it to recognise facial expressions)
	for i in 0 to 137 {
		face = all-faces[i]
		chunk-name = "face" + i
		expression = face[0]
		image = face[1]
		add-dm(chunk-name, "face-expression", expression, image)		
		
		// set activation of face depending on its mood
		face_activation_happy = 1
		face_activation_neutral = 0.4
		face_activation_sad = 0.4
		
		if (expression == "happy") {
			set-activation(chunk-name, face_activation_happy)
		}
		
		if (expression == "neutral") {
			set-activation(chunk-name, face_activation_neutral)
		}
		
		if (expression == "sad") {
			set-activation(chunk-name, face_activation_sad)
		}
	}
		
	all-faces = shuffle(all-faces)
		
	
	// Create 3 blocks of trials, one for each target expression
	blocks = []
	
	happy-target-block = []
	neutral-target-block = []
	sad-target-block = []
	
	// Put 43 trials in each of the three blocks
	// Trials are drawn randomly from the complete list
	for i in 0 to 42 {		
		happy-target-block[i] = ["happy", all-faces[i]]
		neutral-target-block[i] = ["neutral", all-faces[i + 43]]
		sad-target-block[i] = ["sad", all-faces[i + 86]]
	}
	
	
	// Randomly determine the order in which the three blocks are presented
	blocks[0] = happy-target-block
	blocks[1] = neutral-target-block
	blocks[2] = sad-target-block
	
	blocks = shuffle(blocks)
	
	if (verbose) {
		print("Setting up 0-back task...")
		print("Randomly created 3 blocks of trials:")
		blocknum = 1
		for block in blocks {
			print()
			print("BLOCK ", blocknum)
			blocknum = blocknum + 1
			print("______________________________________")
			print("TARGET      EXPRESSION        FACE")
			print("______________________________________")
			trialnum = 0
			expressions = [0, 0, 0]
			expr = ""
			for trial in block {
				target = trial[0]
				expression_face = trial[1]
				expression = expression_face[0]
				face = expression_face[1]
				if (expression == "happy") {
					expressions[0] = expressions[0] + 1
					expr = "hap"
				}
				if (expression == "neutral") {
					expressions[1] = expressions[1] + 1
					expr = "neu"
				}
				if (expression == "sad") {
					expressions[2] = expressions[2] + 1
					expr = "sad"
				}
				print(target, "            ", expr, "                   ", face)
				
				trialnum = trialnum + 1
			}
			print("______________________________________")
			print("(", trialnum, "trials. Happy:", expressions[0], " Neutral:", expressions[1], " Sad:", expressions[2], ")")
			print("______________________________________")
		}
	}
	
	
	// Create neutral off-task memories (Added 10/01/2018)
	// Slots:
	// title (e.g. memory1): for convenience.
	// memory: indicates chunk type; used as first retrieval cue.
	// self: used as an additional retrieval cue (without it we get weird retrievals of random singletons due to partial matching).
	// name (e.g. memory1): distinguishes one memory from another (the chunk title alone is not sufficient).
	
	for i in 0 to 30 {
		title = "memory" + i
		add-dm(title, "memory", "self", title)
	}
	
	
	// Set operator activation
	task-operator-activation = 2
	mw-operator-activation = 2
	
	task-operators = [
		"store-target",
		"process-face",
		"face-expression-matches-target",
		"face-expression-does-not-match-target",
		"face-expression-not-recognised-respond-same",
		"face-expression-not-recognised-respond-diff",
		"done-subvocalise-target",
		"focus-on-fixation"
	]
	
	for task-op in task-operators { 
		set-activation(task-op, task-operator-activation)
	}
	
	mw-operators = [
		"ignore-face-wander-instead",
		"ignore-expression-wander-instead",
		"done-retrieve-random-memory",
		"process-memory-retrieve-next"//,
		//"process-memory-auto-respond-same",
		//"process-memory-auto-respond-diff"
	]
	
	for mw-op in mw-operators {
		set-activation(mw-op, mw-operator-activation)
	}	
	
	
	
	//////////////////////
	/// RUN EXPERIMENT ///
	//////////////////////
	
	if (verbose) {
		print()
		print("Running experiment...")
	}

	// The experiment consists of 3 blocks
	blocknum = 0
	for block in blocks {
		
		blocknum = blocknum + 1
		
		// Each block starts with the presentation of the target expression (which we extract from the first trial)
		first-trial = block[0]
		block-target = first-trial[0]
				
		if (verbose) {
			print("______________________________________")
			print("Starting block", blocknum, ". Target expression: ", block-target)
			print("______________________________________")
		}
		
		screen("target", block-target)
		
		// Show the target until the model presses start
		run-until-action("press-key")
		
		// Each block contains 43 trials
		trialnum = 0
		for trial in block {
			trialnum = trialnum + 1
			face = trial[1]
			expression = face[0]
			image = face[1]
			response = "none"
			correct = 0
			
			trial-matches-target = 0
			if (block-target == expression) {
				trial-matches-target = 1
			}
						
			starttime = time()
			
			if (verbose) {
				print("______________________________________")
				print("Trial", trialnum, "target:", block-target, "current:", expression, time())
				print("______________________________________")
			}
				
			// show face stimulus					
			screen("face", image)
			
			// The face stimulus is on screen for 2.0 seconds
			run-until-relative-time-or-action(2.0, "press-key")
			rt = time() - starttime
			timeleft = 2.0 - rt
			if (verbose) {
				print("----- TIME LEFT: ", timeleft)
			}
			
			// Retrieve the model's latest action (which contains its response or lack thereof)
			action = last-action()
			if (length(action) > 1) {
				response = action[1]				
			}
			if (verbose) {
				print("Action:", response)
			}
			
			
			// If the model was done running before the 2 seconds were up, it responded on time
			if (timeleft > 0) {
				
				// Assess the correctness of the response
				if (response == "same") {
					if (trial-matches-target) {
						// correctly said "same"
						correct = 1
					}
				} else {
					if (!trial-matches-target) {
						// correctly said "diff"
						correct = 1
					}
				}
				
				
				// Show "done" on the screen so that the model does not respond again
				screen("face", image, "done")
				
				// Run the model for the remainder of the two seconds
				run-relative-time(timeleft)
			
			} else {
				// The model did not respond within 2 seconds
				if (verbose) {
					print("No response within 2s time limit.")
				}
			}
			
			if(verbose) {
				if (correct) {
					print("Correct!")
				} else {
					print("Wrong!")
				}
			}
			
			// There is a 0.5 second interval between trials, in which the screen is empty (here fixation)
			screen("fixation")

			run-relative-time(0.5)
						
		}
	}	
}